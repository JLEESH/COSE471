import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from collections import Counter
import random
import argparse
import pickle


# some code slapped together to visualise the effect of subsampling
class Corpus():
    def __init__(self):
        #self.process_text("text8", "processed_corpus") # comment out if the corpus has already been processed
        self.load_corpus("processed_corpus")
        self.corpus_size = len(self.corpus)
        self.vocabulary_size = len(self.word2ind)
        self.debug = True


    '''
    process_text(filename, pickle_filename)


    Preprocesses corpus given in filename to generate and save certain variables into pickle_filename.
    '''
    def process_text(self, filename, pickle_filename):
        # dependencies
        #from collections import Counter
        #import pickle

        print("loading corpus...")
        corpus = open(filename, mode='r')
        corpus = corpus.readlines()[0]  # [:10000]
        corpus = corpus.split()
        corpus_count = Counter(corpus)

        threshold = 4
        corpus_count_processed = {}
        print("creating occurrence dictionary...")
        for word in corpus_count:
            if corpus_count[word] > threshold:
                corpus_count_processed[word] = corpus_count[word]


        # create a processed corpus that contains all the words in the dictionary
        print("processing corpus...")
        corpus_processed = [word for word in corpus if word in corpus_count_processed]

        word2ind = dict()
        ind2word = dict()
        print("creating word2ind and ind2word...")
        for index, word in enumerate(corpus_count.most_common()):
            word2ind[word[0]] = index
            ind2word[index] = word[0]

        bindex = 0
        n_words = 20
        print("taking a peek at word2ind...")
        print()
        for i in range(bindex, bindex + n_words):
            print(ind2word[i], ":", (word2ind[ind2word[i]]))

        print()
        print("original corpus length: ", len(corpus))
        print("reduced corpus length: ", len(corpus_processed))
        print("original vocabulary size: ", len(corpus_count))
        print("reduced vocabulary size: ", len(corpus_count_processed))
        print()


        # save variables using pickle
        with open(pickle_filename, mode="wb") as f:
            pickle.dump(corpus_processed, f)
            pickle.dump(corpus_count_processed, f)
            pickle.dump(word2ind, f)
            pickle.dump(ind2word, f)



    '''
    load_corpus(filename)


    Loads corpus, occurrence_dict, word2ind and ind2word.
    
    Note: requires a pickle file generated by the appropriate preprocessor.
    '''
    def load_corpus(self, filename):
        with open(filename, "rb") as f:
            self.corpus = pickle.load(f)
            self.occurrence_dict = pickle.load(f)
            self.word2ind = pickle.load(f)
            self.ind2word = pickle.load(f)



    '''
    subsample_corpus()

    Subsamples the corpus.
    '''
    def subsample_corpus(self, threshold):

        # initiate dictionaries
        frequency_dict = {}
        discard_probability = {}

        # calculate discard probability for each word in the vocabulary
        for index in range(self.vocabulary_size):
            word = self.ind2word[index]
            if word in self.occurrence_dict:
                frequency_dict[word] = self.occurrence_dict[word] / self.corpus_size
                discard_probability[word] = 1 - (threshold / frequency_dict[word]) ** 0.5
        
        # create new corpus
        new_corpus = [word for word in self.corpus if random.uniform(0, 1) >  discard_probability[word]]
        
        # count new corpus
        new_corpus_count = Counter(new_corpus)

        if self.debug:

            # output debug information
            debug_word_list = ["of", "the", "lucky", "help", self.ind2word[10], self.ind2word[100], self.ind2word[1000], self.ind2word[10000]]
            print("subsampled corpus size:\t", len(new_corpus))
            print("original corpus size:\t", self.corpus_size)
            for word in debug_word_list:
                print("number of instances of", '\"' + word + '\"', "(old):\t", self.occurrence_dict[word])
                print("number of instances of", '\"' + word + '\"', "(new):\t", new_corpus_count[word])
        

        # save new corpus and its length
        self.corpus = new_corpus
        self.corpus_size = len(new_corpus)
        self.occurrence_dict = new_corpus_count



def main():


    model = Corpus()

    # Assignment 4:
    # analyse the proportion of the n most frequent words
    # note that the most frequent 50 words take up more than 40% of the corpus,
    # which is why subsampling/removal of common words may be strongly required
    # for proper training.
    
    # Assignment 5:
    # when the model is subsampled, the proportion of the most common words decreases significantly

    if False:
        print()
        for n in [10, 50, 100, 300, 500, 2000, 3000]:
            print("The top", n, "most frequent words: ")
            print("occurrence count:\t", sum([model.occurrence_dict[model.ind2word[i]] for i in range(0, n)]))
            print("length of corpus:\t", len(model.corpus))
            print("percentage:\t\t %.3f" % (sum([model.occurrence_dict[model.ind2word[i]] for i in range(0, n)]) / len(model.corpus) * 100), "%")
            print()

        print("total vocabulary size:", len(model.occurrence_dict))
        print()
    print()

    # analyse the effect of the threshold
    if True:
        import matplotlib.pyplot as plt
        
        plt.figure()
        thresholds = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]
        lines = {}

        # create a plot for each corpus (note: very inefficient, but convenient)
        for t in thresholds:
            print("creating plot for t=" + str(t) + " ...")

            # reset corpus
            model.load_corpus("processed_corpus")

            # subsample corpus
            model.subsample_corpus(t)

            # x-axis values
            x = [x for x in range(0, 1000, 100)]
            x = x + [a for a in range(1000, 10000, 100)]
            x = x + [b for b in range(10000, 72000, 1000)]
            y = []

            # calculate cumulative percentage of the most common x words in the corpus
            for n in x:
                if n in model.ind2word and model.ind2word[n] in model.corpus:
                        y.append(sum([model.occurrence_dict[model.ind2word[i]] for i in range(0, n)]) / len(model.corpus) * 100)
                else:
                    y.append(y[-1]) # not a fail-safe way but significantly quicker

            label = "t=" + str(t)
            lines[t],  = plt.plot(x, y, label=label)
            print()

        plt.legend()
        plt.show()


if __name__ == '__main__':
    main()
